{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442915b-c315-4c20-b39b-4c7547f39041",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7aa8ca-cc4e-45f5-9a8f-e6615c3cdb3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51654e-a0cb-4d03-97ac-c979eb998563",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b1d45-5939-4802-8ac6-51d98adf5213",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea379814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d776fc-45b6-4dfa-99d9-a8ae5a02dbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "aws_access_key_id='YOUR ACCESS KEY',\n",
    "aws_secret_access_key='YOUR-SECRET',\n",
    "\n",
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210826de-0c89-4f28-9d22-aa744edcb9dc",
   "metadata": {},
   "source": [
    "# 1. Create Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd12296-47ae-450f-a978-016902096611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket_name, prefix=''):\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    if 'Contents' in response:\n",
    "        return [obj['Key'] for obj in response['Contents']]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d98d8bdd-1d80-4f5f-bca4-699760a32d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_from_s3(s3_path, local_path):\n",
    "    \"\"\"\n",
    "    Download a file from S3 to local path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        s3_client.download_file('YOUR-BUCKET-NAME', s3_path, local_path)\n",
    "         \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from S3: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73545586-8876-4940-852e-44c36a1f876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_json(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Load large JSON file using Dask for parallel processing\n",
    "    \"\"\"\n",
    "    s3_path = f's3://{bucket_name}/{file_key}'\n",
    "    \n",
    "    if file_key.endswith('.jsonl') or file_key.endswith('.ndjson'):\n",
    "        df = pd.read_json(s3_path, lines=True)\n",
    "    else:\n",
    "        # For regular JSON, might need to preprocess the file\n",
    "        df = pd.read_json(s3_path)\n",
    "    \n",
    "    # Compute the final DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b40d7a3e-25b8-47f6-b3e0-4436f170f046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_pdf_to_single_image(pdf_path):\n",
    "    # Convert PDF to list of images\n",
    "    images = convert_from_path(pdf_path, dpi=80)\n",
    "    \n",
    "    if not images:\n",
    "        return None\n",
    "        \n",
    "    if len(images) == 1:\n",
    "        return images[0]\n",
    "    \n",
    "    # Calculate total height and max width\n",
    "    total_height = sum(img.height for img in images)\n",
    "    max_width = max(img.width for img in images)\n",
    "    \n",
    "    # Create new image with combined height\n",
    "    combined_image = Image.new('L', (max_width, total_height), 'white')\n",
    "    \n",
    "    # Paste images vertically\n",
    "    y_offset = 0\n",
    "    for img in images:\n",
    "        combined_image.paste(img, (0, y_offset))\n",
    "        y_offset += img.height\n",
    "    \n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8283ab8-1fa5-495c-8626-8496e6e3ea7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"You are an expert of extracting fines and tolls informations from invoices.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    conversation = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        { \"role\" : \"assistant\",\n",
    "          \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : sample[\"caption\"]} ]\n",
    "        },\n",
    "    ]\n",
    "    return { \"messages\" : conversation }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9556e-b74d-4a1b-83b8-f3eb21443f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = load_json(PATH, FILE_NAME)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53120734-d811-43d4-a4b0-338707cae9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = ['license_plate', 'event_date', 'amount']\n",
    "\n",
    "def format_row(row):\n",
    "    return '{' + ', '.join([\n",
    "        f\"'license_plate':'{row['license_plate']}'\",\n",
    "        f\"'event_date':'{row['event_date']}'\",\n",
    "        f\"'amount':'{row['amount']:.2f}'\"\n",
    "    ]) + '}'\n",
    "\n",
    "df['caption'] = df.apply(format_row, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab04a00e-ab34-4956-b541-3da322f4b117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('pdf_path').agg({\n",
    "    'caption': list\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f83fb1a-6942-4910-b812-6fde85126b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df['caption'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a96cbc07-837c-4b63-b7fb-73140592be9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_list = list_s3_objects('YOUR-BUCKET-NAME', 'LOCAL-PDF-FOLDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980491aa-086a-4f8a-a409-458b063bbdc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_list[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd0d57-d7c3-44bb-984f-b49ffa487d7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_images = []\n",
    "main_image_ids = []\n",
    "main_image_caption = []\n",
    "\n",
    "for file in document_list:\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] INFO: Processing File: {file}\")\n",
    "\n",
    "    pdf_name = file.split('/')[-1]\n",
    "    download_from_s3(file, f\"./pdfs/{pdf_name}\")\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] INFO: Local copy with Success: {pdf_name}\")\n",
    "    \n",
    "    try:\n",
    "        images = convert_pdf_to_single_image(f\"./pdfs/{pdf_name}\")\n",
    "    except:\n",
    "        images = None\n",
    "    \n",
    "    if images:\n",
    "\n",
    "        caption = grouped_df[grouped_df.pdf_path == pdf_name]['caption'].values\n",
    "        \n",
    "        main_images.append(images)\n",
    "        main_image_ids.append(pdf_name)\n",
    "        main_image_caption.append(caption)\n",
    "\n",
    "        images.save(f'./images/agg_{pdf_name}' + '.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cbde6-eb58-4af9-8977-26b4e0aa3019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cf83c68-df94-4a01-9a44-65dedf507467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = list(map(\n",
    "    lambda x: {'image': x[0], 'image_id': x[1], 'caption': x[2]},\n",
    "    #list(Zip()) creates a list of 3 elements following the order in the input lists\n",
    "    zip(main_images, main_image_ids, main_image_caption)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c421705-743c-48c5-846a-05e9037d85eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converted_dataset = [convert_to_conversation(sample) for sample in dataset]\n",
    "converted_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913dff03-7cf6-481a-bec5-5b9aaf74017c",
   "metadata": {},
   "source": [
    "# 2. Trainning Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4c26d-8760-4e49-a8cf-f16e83387319",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
    "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
    "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
    "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
    "\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
    "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
    "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ec124-3295-408a-af31-2d9d904b699e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b19c631f-852e-4888-9749-eebced2d700a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = converted_dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 100,\n",
    "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 1024,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90654e53-20d5-4e2f-8e83-ef22c29c44e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbefe54-a363-4d7d-8b75-1704b8b5135e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e3a55-f92f-40ed-92ec-cbdc872a675d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc04952b-af7d-4df2-9ccb-ddf81b1d5fa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d01a2-0597-415f-90f9-23b8c60500cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = dataset[2][\"image\"]\n",
    "\n",
    "instruction = \"You are an expert of extracting fines and tolls informations from invoices.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 618,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e54861-bb7b-4bd5-ba0c-4133aad9d8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985243b9-a172-40f2-9b8a-7b5bcb6d5dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_llm_finetune_p310",
   "language": "python",
   "name": "unsloth_llm_finetune_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
